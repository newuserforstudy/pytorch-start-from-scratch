{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 关于pytorch中自动微分的几个细节\n",
    "(1)设置tensor的属性：requires_grad=True。    \n",
    "(2)默认tensor的属性grad会累加，一般在再次反向传播之前要清0，x.grad.data.zero_()。     \n",
    "(3)在某个时间不再追踪梯度，使用detach函数分离。    \n",
    "(4)输出结果为向量或者更高维张量时，backward()要传参，参数类型与输出结果一致，输出结果为标量可以不传参。    \n",
    "(5)输出为向量或者或者更高维张量时传参的目的：防止张量对张量求导，只允许标量对张量求导，求导结果是与自变量同型的张量。     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 pytorch中的自动求导机制与流程   \n",
    "step1: 执行反向传播backward()时，该操作将调用tensor的grad_fn属性，grad_fn属性指向一个Function类的对象即创建该tensor的方法，记录并且编码了完整的计算历史。    \n",
    "step2：在PyTorch的反向图计算中，AccumulateGrad类型代表的就是叶子节点类型，即计算图终止节点。AccumulateGrad类中有一个variable属性指向叶子节点。  \n",
    "step3: 由于grad_fn是一个Function类型的对象，next_function属性是grad_fn对象的一个重要的属性，next_function属性是一个元组，里面存放是关于所有自变量的操作记录，包括variable属性。   \n",
    "step4:执行反向传播backward()操作将递归地遍历grad_fn的next_functions属性中的所有记录，然后分别取出里面存放的Function（AccumulateGrad），执行求导操作。  \n",
    "step5：求导计算结果保存到他们对应tensor的grad属性中。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
