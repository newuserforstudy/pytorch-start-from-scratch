{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 torch.nn模块简介 \n",
    "\n",
    "包含定义神经网络过程中的用到的网络模型、参数、损失函数、优化方法等等  \n",
    "\n",
    "官网：https://pytorch.org/docs/stable/nn.html  以及 https://pytorch.org/tutorials/beginner/nn_tutorial.html  \n",
    "中文参考：https://pytorch.apachecn.org/docs/1.2/nn.html    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 nn模块部分内容介绍  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  torch.nn.Module类\n",
    "Module类是所有神经网络模型的基类，创建自己模型的时候也应该继承这个类。  \n",
    "Module类的源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module  \n",
    "创建自己的神经网络模型时候：   \n",
    "要重写构造\\_\\_init\\_\\_方法和反向传播forward方法   \n",
    "可以通过add_module方法进行网络搭建   \n",
    "可以通过parameters方法返回模型参数的迭代器   \n",
    "更多方法上面提供的源码   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 torch.nn.Parameter类\n",
    "Parameters类是Tensor 的子类, Parameters类有一个很重要的特性就是当其在 Module类中被使用并被当做这个Module类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个Module类的参数列表(list of parameters)之中，同时也就会被添加入此Module类的 parameters()方法所返回的参数迭代器中。  \n",
    "Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非Parameter的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。   \n",
    "torch.nn.Parameter源码：https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 torch.nn.Sequential类\n",
    "一种顺序容器。传入Sequential构造器中的模块会被按照他们传入的顺序依次添加到Sequential之上。相应的，一个由模块组成的顺序词典也可以被传入到Sequential的构造器中。  \n",
    "torch.nn.Sequential源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 torch.nn卷积层\n",
    "torch.nn.Conv1d类  \n",
    "torch.nn.Conv1d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv1d  \n",
    "torch.nn.Conv2d类  \n",
    "torch.nn.Conv2d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d\n",
    "torch.nn.Conv3d类  \n",
    "torch.nn.Conv3d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 torch.nn池化层\n",
    "torch.nn.MaxPool1d类  \n",
    "torch.nn.MaxPool1d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool1d   \n",
    "torch.nn.MaxPool2d类  \n",
    "torch.nn.MaxPool2d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool2d  \n",
    "torch.nn.MaxPool3d类  \n",
    "torch.nn.MaxPool3d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool3d   \n",
    "\n",
    "torch.nn.AvgPool1d类  \n",
    "torch.nn.AvgPool1d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool1d  \n",
    "torch.nn.AvgPool2d类  \n",
    "torch.nn.AvgPool2d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool2d  \n",
    "torch.nn.AvgPool3d类  \n",
    "torch.nn.AvgPool3d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool3d   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 torch.nn填充层\n",
    "torch.nn.ZeroPad2d类  \n",
    "torch.nn.ZeroPad2d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ZeroPad2d    \n",
    "torch.nn.ConstantPad1d类  \n",
    "torch.nn.ConstantPad1d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad1d  \n",
    "torch.nn.ConstantPad2d类  \n",
    "torch.nn.ConstantPad2d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad2d torch.nn.ConstantPad3d类  \n",
    "torch.nn.ConstantPad3d源码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad3d  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 torch.nn非线性激活层   \n",
    "class torch.nn.ELU(alpha=1.0, inplace=False)  \n",
    "class torch.nn.Hardshrink(lambd=0.5)   \n",
    "class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)   \n",
    "class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)  \n",
    "class torch.nn.LogSigmoid   \n",
    "class torch.nn.PReLU(num_parameters=1, init=0.25)  \n",
    "class torch.nn.ReLU(inplace=False)  \n",
    "class torch.nn.ReLU6(inplace=False)  \n",
    "class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)  \n",
    "class torch.nn.SELU(inplace=False)  \n",
    "class torch.nn.CELU(alpha=1.0, inplace=False)  \n",
    "class torch.nn.Sigmoid  \n",
    "class torch.nn.Softplus(beta=1, threshold=20)  \n",
    "class torch.nn.Softshrink(lambd=0.5)\n",
    "class torch.nn.Softsign    \n",
    "class torch.nn.Tanh   \n",
    "class torch.nn.Tanhshrink   \n",
    "class torch.nn.Threshold(threshold, value, inplace=False)  \n",
    "class torch.nn.Softmax(dim=None)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 torch.nn归一化层\n",
    "class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  \n",
    "class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  \n",
    "class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   \n",
    "class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)  \n",
    "class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)  \n",
    "class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 torch.nn循环层\n",
    "class torch.nn.RNN(*args, **kwargs)   \n",
    "class torch.nn.LSTM(*args, **kwargs)  \n",
    "class torch.nn.GRU(*args, **kwargs)  \n",
    "class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')  \n",
    "class torch.nn.LSTMCell(input_size, hidden_size, bias=True)  \n",
    "class torch.nn.GRUCell(input_size, hidden_size, bias=True)   \n",
    "class torch.nn.RNNBase(mode, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 torch.nn Transformer层\n",
    "\n",
    "class torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None)   \n",
    "\n",
    "class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)  \n",
    "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)\n",
    "\n",
    "class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')  \n",
    "class torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 torch.nn Transformer层\n",
    "\n",
    "class torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None)   \n",
    "\n",
    "class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)  \n",
    "class torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)\n",
    "\n",
    "class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')  \n",
    "class torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 torch.nn 线性层\n",
    "\n",
    "class torch.nn.Linear(in_features, out_features, bias=True)  \n",
    "class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 torch.nn dropout层\n",
    "\n",
    "class torch.nn.Dropout(p=0.5, inplace=False)  \n",
    "class torch.nn.Dropout2d(p=0.5, inplace=False)   \n",
    "class torch.nn.Dropout3d(p=0.5, inplace=False)   \n",
    "class torch.nn.AlphaDropout(p=0.5, inplace=False)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 torch.nn 稀疏层\n",
    "\n",
    "class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)  \n",
    "\n",
    "class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 torch.nn距离计算  \n",
    "\n",
    "class torch.nn.CosineSimilarity(dim=1, eps=1e-08)   \n",
    "class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 torch.nn损失计算 \n",
    "\n",
    "class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')   \n",
    "class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')  \n",
    "class torch.nn.CTCLoss(blank=0, reduction='mean')   \n",
    "class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')  \n",
    "class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')  \n",
    "class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')   \n",
    "class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')   \n",
    "class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) \n",
    "class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')  \n",
    "class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')   \n",
    "class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') \n",
    "class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 torch.nn Flatten层\n",
    "class torch.nn.Flatten(start_dim=1, end_dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多内容参考官网：https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
